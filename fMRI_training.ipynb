{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08e252a1f9516987486a0f48518217475b1a96ae470be4b589c7d5bdaf71ddca8",
   "display_name": "Python 3.7.10 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e252a1f9516987486a0f48518217475b1a96ae470be4b589c7d5bdaf71ddca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from utils import *\n",
    "from model.MLP import *\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "input_size = {'15':105,\n",
    "                '25':300,\n",
    "                '50':1225,\n",
    "                '100':4950,\n",
    "                '200':19900,\n",
    "                '300':44850\n",
    "                }\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### TO DO\n",
    "\n",
    "1) Prediction of sex\n",
    "\n",
    "TO CONSIDER:\n",
    "\n",
    "- normalisation (?)\n",
    "- compare different models with the same number of neurons\n",
    "- add HPs in tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/sd20/workspace/data/HCP/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             1         2       3        4       5        6        7       8  \\\n",
       "0      7.49720  23.35000  78.054  -4.9579  43.429  60.4470  41.4040 -7.5742   \n",
       "1     -4.99950   4.73090  54.711  -6.9007  13.804  30.3770  26.1270 -2.1377   \n",
       "2      9.40280   5.24820  99.701 -12.2550  30.410  67.0210  49.9930  1.2277   \n",
       "3    -28.31500 -13.18600  59.763 -24.9000  16.699  44.6980  34.0170 -8.2790   \n",
       "4     -0.14134   8.04310  63.203  -2.1901  24.454  46.9130  31.4790  5.5834   \n",
       "...        ...       ...     ...      ...     ...      ...      ...     ...   \n",
       "998   -0.85452   3.24290  60.144  -6.1011  10.891  55.0160  64.7880 -9.6518   \n",
       "999    3.42850  10.47600  92.195 -12.6510  35.189  47.2660  52.4960  7.3938   \n",
       "1000   4.76000  16.95700  20.941  14.9970  27.368   7.6914   8.8762  1.4107   \n",
       "1001   5.93070  -0.43158  86.431 -17.4260  27.843  64.6700  66.5200 -4.5230   \n",
       "1002  -8.51760   3.63140  46.417  -6.0625  28.845  23.8540  40.2890  7.0224   \n",
       "\n",
       "             9         10  ...     88798    88799    89097    89098     89099  \\\n",
       "0     17.58300  -2.662400  ... -0.658430 -3.31760  2.79060  1.33920  2.231700   \n",
       "1     18.45000 -14.857000  ... -0.780040 -1.70270 -1.00820  0.36915 -0.664390   \n",
       "2      1.40360  -0.461260  ... -3.392200 -0.56904  3.66550  1.26210  2.101300   \n",
       "3      5.16900   0.089982  ...  0.124290 -0.36566 -0.94756 -0.63533  1.234200   \n",
       "4      2.11680   9.282800  ... -1.795900  3.17070  1.39460  0.79006  0.775760   \n",
       "...        ...        ...  ...       ...      ...      ...      ...       ...   \n",
       "998    4.97660 -12.543000  ...  1.257800  2.77780  0.22414 -0.47279  2.964200   \n",
       "999    0.57836   4.123800  ...  0.643660 -2.93160  0.24456  3.25640  1.213900   \n",
       "1000  -1.37530  -8.494300  ... -1.795200 -1.09860  0.55332 -0.11358 -1.547500   \n",
       "1001  -5.20640  15.786000  ...  0.019416 -2.32110  0.79256  0.14387 -0.053458   \n",
       "1002  -2.15310   9.071000  ... -0.015315  0.26386 -0.75828 -2.38870  0.922310   \n",
       "\n",
       "        89398    89399   89699  sex  fluid_intelligence  \n",
       "0     1.51680 -0.28532  4.6379    0                20.0  \n",
       "1     1.45510 -0.76610  4.5984    1                17.0  \n",
       "2     0.78259  1.19860  4.4317    0                 7.0  \n",
       "3     1.27270  0.46160  6.1709    0                23.0  \n",
       "4     0.17996 -2.84420  2.9442    1                11.0  \n",
       "...       ...      ...     ...  ...                 ...  \n",
       "998  -0.26596 -1.32200  7.5611    1                21.0  \n",
       "999  -0.68257  1.07780  4.9371    0                14.0  \n",
       "1000 -0.76885  1.31420  6.4576    1                21.0  \n",
       "1001 -0.63690 -1.50930  5.8259    0                20.0  \n",
       "1002  1.72560 -1.57620  4.3008    1                20.0  \n",
       "\n",
       "[1003 rows x 44852 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>88798</th>\n      <th>88799</th>\n      <th>89097</th>\n      <th>89098</th>\n      <th>89099</th>\n      <th>89398</th>\n      <th>89399</th>\n      <th>89699</th>\n      <th>sex</th>\n      <th>fluid_intelligence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.49720</td>\n      <td>23.35000</td>\n      <td>78.054</td>\n      <td>-4.9579</td>\n      <td>43.429</td>\n      <td>60.4470</td>\n      <td>41.4040</td>\n      <td>-7.5742</td>\n      <td>17.58300</td>\n      <td>-2.662400</td>\n      <td>...</td>\n      <td>-0.658430</td>\n      <td>-3.31760</td>\n      <td>2.79060</td>\n      <td>1.33920</td>\n      <td>2.231700</td>\n      <td>1.51680</td>\n      <td>-0.28532</td>\n      <td>4.6379</td>\n      <td>0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.99950</td>\n      <td>4.73090</td>\n      <td>54.711</td>\n      <td>-6.9007</td>\n      <td>13.804</td>\n      <td>30.3770</td>\n      <td>26.1270</td>\n      <td>-2.1377</td>\n      <td>18.45000</td>\n      <td>-14.857000</td>\n      <td>...</td>\n      <td>-0.780040</td>\n      <td>-1.70270</td>\n      <td>-1.00820</td>\n      <td>0.36915</td>\n      <td>-0.664390</td>\n      <td>1.45510</td>\n      <td>-0.76610</td>\n      <td>4.5984</td>\n      <td>1</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.40280</td>\n      <td>5.24820</td>\n      <td>99.701</td>\n      <td>-12.2550</td>\n      <td>30.410</td>\n      <td>67.0210</td>\n      <td>49.9930</td>\n      <td>1.2277</td>\n      <td>1.40360</td>\n      <td>-0.461260</td>\n      <td>...</td>\n      <td>-3.392200</td>\n      <td>-0.56904</td>\n      <td>3.66550</td>\n      <td>1.26210</td>\n      <td>2.101300</td>\n      <td>0.78259</td>\n      <td>1.19860</td>\n      <td>4.4317</td>\n      <td>0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-28.31500</td>\n      <td>-13.18600</td>\n      <td>59.763</td>\n      <td>-24.9000</td>\n      <td>16.699</td>\n      <td>44.6980</td>\n      <td>34.0170</td>\n      <td>-8.2790</td>\n      <td>5.16900</td>\n      <td>0.089982</td>\n      <td>...</td>\n      <td>0.124290</td>\n      <td>-0.36566</td>\n      <td>-0.94756</td>\n      <td>-0.63533</td>\n      <td>1.234200</td>\n      <td>1.27270</td>\n      <td>0.46160</td>\n      <td>6.1709</td>\n      <td>0</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.14134</td>\n      <td>8.04310</td>\n      <td>63.203</td>\n      <td>-2.1901</td>\n      <td>24.454</td>\n      <td>46.9130</td>\n      <td>31.4790</td>\n      <td>5.5834</td>\n      <td>2.11680</td>\n      <td>9.282800</td>\n      <td>...</td>\n      <td>-1.795900</td>\n      <td>3.17070</td>\n      <td>1.39460</td>\n      <td>0.79006</td>\n      <td>0.775760</td>\n      <td>0.17996</td>\n      <td>-2.84420</td>\n      <td>2.9442</td>\n      <td>1</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>-0.85452</td>\n      <td>3.24290</td>\n      <td>60.144</td>\n      <td>-6.1011</td>\n      <td>10.891</td>\n      <td>55.0160</td>\n      <td>64.7880</td>\n      <td>-9.6518</td>\n      <td>4.97660</td>\n      <td>-12.543000</td>\n      <td>...</td>\n      <td>1.257800</td>\n      <td>2.77780</td>\n      <td>0.22414</td>\n      <td>-0.47279</td>\n      <td>2.964200</td>\n      <td>-0.26596</td>\n      <td>-1.32200</td>\n      <td>7.5611</td>\n      <td>1</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>3.42850</td>\n      <td>10.47600</td>\n      <td>92.195</td>\n      <td>-12.6510</td>\n      <td>35.189</td>\n      <td>47.2660</td>\n      <td>52.4960</td>\n      <td>7.3938</td>\n      <td>0.57836</td>\n      <td>4.123800</td>\n      <td>...</td>\n      <td>0.643660</td>\n      <td>-2.93160</td>\n      <td>0.24456</td>\n      <td>3.25640</td>\n      <td>1.213900</td>\n      <td>-0.68257</td>\n      <td>1.07780</td>\n      <td>4.9371</td>\n      <td>0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>4.76000</td>\n      <td>16.95700</td>\n      <td>20.941</td>\n      <td>14.9970</td>\n      <td>27.368</td>\n      <td>7.6914</td>\n      <td>8.8762</td>\n      <td>1.4107</td>\n      <td>-1.37530</td>\n      <td>-8.494300</td>\n      <td>...</td>\n      <td>-1.795200</td>\n      <td>-1.09860</td>\n      <td>0.55332</td>\n      <td>-0.11358</td>\n      <td>-1.547500</td>\n      <td>-0.76885</td>\n      <td>1.31420</td>\n      <td>6.4576</td>\n      <td>1</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>5.93070</td>\n      <td>-0.43158</td>\n      <td>86.431</td>\n      <td>-17.4260</td>\n      <td>27.843</td>\n      <td>64.6700</td>\n      <td>66.5200</td>\n      <td>-4.5230</td>\n      <td>-5.20640</td>\n      <td>15.786000</td>\n      <td>...</td>\n      <td>0.019416</td>\n      <td>-2.32110</td>\n      <td>0.79256</td>\n      <td>0.14387</td>\n      <td>-0.053458</td>\n      <td>-0.63690</td>\n      <td>-1.50930</td>\n      <td>5.8259</td>\n      <td>0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>-8.51760</td>\n      <td>3.63140</td>\n      <td>46.417</td>\n      <td>-6.0625</td>\n      <td>28.845</td>\n      <td>23.8540</td>\n      <td>40.2890</td>\n      <td>7.0224</td>\n      <td>-2.15310</td>\n      <td>9.071000</td>\n      <td>...</td>\n      <td>-0.015315</td>\n      <td>0.26386</td>\n      <td>-0.75828</td>\n      <td>-2.38870</td>\n      <td>0.922310</td>\n      <td>1.72560</td>\n      <td>-1.57620</td>\n      <td>4.3008</td>\n      <td>1</td>\n      <td>20.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1003 rows × 44852 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('./data/netmats/ICA300/connectivity_ICA300.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape X train (802, 105) , y train (802,)\nshape X val (201, 105) , y val (201,)\n"
     ]
    }
   ],
   "source": [
    "nbr_components= 15\n",
    "\n",
    "loader = dataLoaderICA(data_path = './data/netmats',\n",
    "                        nbr_components = nbr_components,\n",
    "                        batch_size = 32,\n",
    "                        num_workers = 8,\n",
    "                        label ='sex',)\n",
    "\n",
    "dataloaders = loader.getDataLoaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\ntorch.Size([32, 105]) torch.Size([32])\n1\ntorch.Size([32, 105]) torch.Size([32])\n2\ntorch.Size([32, 105]) torch.Size([32])\n3\ntorch.Size([32, 105]) torch.Size([32])\n4\ntorch.Size([32, 105]) torch.Size([32])\n5\ntorch.Size([32, 105]) torch.Size([32])\n6\ntorch.Size([32, 105]) torch.Size([32])\n7\ntorch.Size([32, 105]) torch.Size([32])\n8\ntorch.Size([32, 105]) torch.Size([32])\n9\ntorch.Size([32, 105]) torch.Size([32])\n10\ntorch.Size([32, 105]) torch.Size([32])\n11\ntorch.Size([32, 105]) torch.Size([32])\n12\ntorch.Size([32, 105]) torch.Size([32])\n13\ntorch.Size([32, 105]) torch.Size([32])\n14\ntorch.Size([32, 105]) torch.Size([32])\n15\ntorch.Size([32, 105]) torch.Size([32])\n16\ntorch.Size([32, 105]) torch.Size([32])\n17\ntorch.Size([32, 105]) torch.Size([32])\n18\ntorch.Size([32, 105]) torch.Size([32])\n19\ntorch.Size([32, 105]) torch.Size([32])\n20\ntorch.Size([32, 105]) torch.Size([32])\n21\ntorch.Size([32, 105]) torch.Size([32])\n22\ntorch.Size([32, 105]) torch.Size([32])\n23\ntorch.Size([32, 105]) torch.Size([32])\n24\ntorch.Size([32, 105]) torch.Size([32])\n25\ntorch.Size([2, 105]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloaders['train']):\n",
    "    print(i)\n",
    "    print(data[0].shape, data[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_layers): ModuleDict(\n",
       "    (fc_layer_0): Linear(in_features=300, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc_class): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "net = MLP(input_size=300,\n",
    "          hidden_size=[16],\n",
    "          dropout=0.5)\n",
    "\n",
    "net"
   ]
  },
  {
   "source": [
    "#### Optimiser, Loss and Training parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_components = 25\n",
    "\n",
    "input_size = 300\n",
    "hidden_size =[16]\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "lr = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape X train (802, 300) , y train (802,)\nshape X val (201, 300) , y val (201,)\n"
     ]
    }
   ],
   "source": [
    "# loaders\n",
    "loader = dataLoaderICA(data_path = './data/netmats',\n",
    "                        nbr_components = nbr_components,\n",
    "                        batch_size = 32,\n",
    "                        num_workers = 8,\n",
    "                        label ='sex',)\n",
    "\n",
    "\n",
    "dataloaders = loader.getDataLoaders()\n",
    "\n",
    "# network\n",
    "net = MLP(input_size,\n",
    "          hidden_size,\n",
    "          dropout)\n",
    "\n",
    "#loss\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#optimiser\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "#lr scheduler\n",
    "\n",
    "#metrics to store during training\n",
    "metrics = {'loss_train': [], 'loss_valid': [], \n",
    "           'acc_train': [], 'acc_valid':[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### logging\n",
    "\n",
    "1) save parameters\n",
    "\n",
    "2) create folder for log tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-05-18\nfolder already exist: /home/sd20/workspace/spatio-temporal-fmri-analysis/logs/2021-05-18\n13:15:18\nCreating folder: /home/sd20/workspace/spatio-temporal-fmri-analysis/logs/2021-05-18/ICA_25/13:15:18\n"
     ]
    }
   ],
   "source": [
    "path = '/home/sd20/workspace/spatio-temporal-fmri-analysis'\n",
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "print(date)\n",
    "\n",
    "# folder date\n",
    "folder_to_save_model = os.path.join(path,'logs', date)\n",
    "try:\n",
    "    os.mkdir(folder_to_save_model)\n",
    "    print('Creating folder: {}'.format(folder_to_save_model))\n",
    "except OSError:\n",
    "    print('folder already exist: {}'.format(folder_to_save_model))\n",
    "\n",
    "date = datetime.today().strftime('%H:%M:%S')\n",
    "print(date)\n",
    "\n",
    "# folder ICA\n",
    "folder_to_save_model = os.path.join(folder_to_save_model,'ICA_{}'.format(nbr_components))\n",
    "\n",
    "# folder time\n",
    "folder_to_save_model = os.path.join(folder_to_save_model,date)\n",
    "try:\n",
    "    os.mkdir(folder_to_save_model)\n",
    "    print('Creating folder: {}'.format(folder_to_save_model))\n",
    "except OSError:\n",
    "    print('folder already exist: {}'.format(folder_to_save_model))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tensorboard \n",
    "writer = SummaryWriter(log_dir=folder_to_save_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(folder_to_save_model,'log.txt'),\"w+\")\n",
    "\n",
    "# logging the hyperparameters\n",
    "\n",
    "f.write(\"Batch size: {} \\n\".format(batch_size))\n",
    "f.write(\"Epoch: {} \\n\".format(n_epochs))\n",
    "f.write(\"Learning rate: {} \\n\".format(lr))\n",
    "f.write(\"Optimiser: {} \\n\".format(optimiser))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'13:15:19'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "date = datetime.today().strftime('%H:%M:%S')\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_layers): ModuleDict(\n",
       "    (fc_layer_0): Linear(in_features=300, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc_class): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, torch.rand(batch_size,300))"
   ]
  },
  {
   "source": [
    "#### TRAINING & VALIDATION LOOPS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "##################################################\n",
      "412.0 802.0\n",
      "TRAIN Epoch:   0 | Acc:   51| Loss:6.4960e-01 | 0m 0.23s\n",
      "\n",
      "/home/sd20/anaconda3/envs/fMRI/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "113.0 201.0\n",
      "VALID Epoch:   0 | Acc:   56| Loss:5.2705e-01 | 0m 0.39s\n",
      "\n",
      "##################################################\n",
      "424.0 802.0\n",
      "TRAIN Epoch:   1 | Acc:   52| Loss:6.0548e-01 | 0m 0.21s\n",
      "\n",
      "118.0 201.0\n",
      "VALID Epoch:   1 | Acc:   58| Loss:5.3747e-01 | 0m 0.37s\n",
      "\n",
      "##################################################\n",
      "475.0 802.0\n",
      "TRAIN Epoch:   2 | Acc:   59| Loss:5.6312e-01 | 0m 0.19s\n",
      "\n",
      "123.0 201.0\n",
      "VALID Epoch:   2 | Acc:   61| Loss:5.0494e-01 | 0m 0.35s\n",
      "\n",
      "##################################################\n",
      "495.0 802.0\n",
      "TRAIN Epoch:   3 | Acc:   61| Loss:7.2257e-01 | 0m 0.19s\n",
      "\n",
      "125.0 201.0\n",
      "VALID Epoch:   3 | Acc:   62| Loss:4.9119e-01 | 0m 0.34s\n",
      "\n",
      "##################################################\n",
      "529.0 802.0\n",
      "TRAIN Epoch:   4 | Acc:   65| Loss:5.4175e-01 | 0m 0.19s\n",
      "\n",
      "125.0 201.0\n",
      "VALID Epoch:   4 | Acc:   62| Loss:4.8430e-01 | 0m 0.35s\n",
      "\n",
      "##################################################\n",
      "553.0 802.0\n",
      "TRAIN Epoch:   5 | Acc:   68| Loss:6.7220e-01 | 0m 0.19s\n",
      "\n",
      "131.0 201.0\n",
      "VALID Epoch:   5 | Acc:   65| Loss:4.9813e-01 | 0m 0.35s\n",
      "\n",
      "##################################################\n",
      "564.0 802.0\n",
      "TRAIN Epoch:   6 | Acc:   70| Loss:7.3551e-01 | 0m 0.22s\n",
      "\n",
      "130.0 201.0\n",
      "VALID Epoch:   6 | Acc:   64| Loss:4.9501e-01 | 0m 0.39s\n",
      "\n",
      "##################################################\n",
      "592.0 802.0\n",
      "TRAIN Epoch:   7 | Acc:   73| Loss:2.6626e-01 | 0m 0.22s\n",
      "\n",
      "142.0 201.0\n",
      "VALID Epoch:   7 | Acc:   70| Loss:4.8684e-01 | 0m 0.40s\n",
      "\n",
      "##################################################\n",
      "599.0 802.0\n",
      "TRAIN Epoch:   8 | Acc:   74| Loss:5.8956e-01 | 0m 0.19s\n",
      "\n",
      "134.0 201.0\n",
      "VALID Epoch:   8 | Acc:   66| Loss:4.8993e-01 | 0m 0.36s\n",
      "\n",
      "##################################################\n",
      "608.0 802.0\n",
      "TRAIN Epoch:   9 | Acc:   75| Loss:4.4691e-01 | 0m 0.20s\n",
      "\n",
      "141.0 201.0\n",
      "VALID Epoch:   9 | Acc:   70| Loss:4.7910e-01 | 0m 0.37s\n",
      "\n",
      "##################################################\n",
      "617.0 802.0\n",
      "TRAIN Epoch:  10 | Acc:   76| Loss:4.0330e-01 | 0m 0.18s\n",
      "\n",
      "140.0 201.0\n",
      "VALID Epoch:  10 | Acc:   69| Loss:4.8413e-01 | 0m 0.34s\n",
      "\n",
      "##################################################\n",
      "633.0 802.0\n",
      "TRAIN Epoch:  11 | Acc:   78| Loss:2.6240e-01 | 0m 0.20s\n",
      "\n",
      "143.0 201.0\n",
      "VALID Epoch:  11 | Acc:   71| Loss:4.7513e-01 | 0m 0.36s\n",
      "\n",
      "##################################################\n",
      "638.0 802.0\n",
      "TRAIN Epoch:  12 | Acc:   79| Loss:5.3828e-01 | 0m 0.21s\n",
      "\n",
      "143.0 201.0\n",
      "VALID Epoch:  12 | Acc:   71| Loss:4.6294e-01 | 0m 0.36s\n",
      "\n",
      "##################################################\n",
      "644.0 802.0\n",
      "TRAIN Epoch:  13 | Acc:   80| Loss:6.4564e-02 | 0m 0.19s\n",
      "\n",
      "148.0 201.0\n",
      "VALID Epoch:  13 | Acc:   73| Loss:4.5151e-01 | 0m 0.37s\n",
      "\n",
      "##################################################\n",
      "647.0 802.0\n",
      "TRAIN Epoch:  14 | Acc:   80| Loss:7.4984e-01 | 0m 0.22s\n",
      "\n",
      "150.0 201.0\n",
      "VALID Epoch:  14 | Acc:   74| Loss:4.4226e-01 | 0m 0.38s\n",
      "\n",
      "##################################################\n",
      "654.0 802.0\n",
      "TRAIN Epoch:  15 | Acc:   81| Loss:4.7700e-01 | 0m 0.20s\n",
      "\n",
      "147.0 201.0\n",
      "VALID Epoch:  15 | Acc:   73| Loss:4.4298e-01 | 0m 0.37s\n",
      "\n",
      "##################################################\n",
      "659.0 802.0\n",
      "TRAIN Epoch:  16 | Acc:   82| Loss:2.2988e-01 | 0m 0.23s\n",
      "\n",
      "151.0 201.0\n",
      "VALID Epoch:  16 | Acc:   75| Loss:4.3191e-01 | 0m 0.38s\n",
      "\n",
      "##################################################\n",
      "663.0 802.0\n",
      "TRAIN Epoch:  17 | Acc:   82| Loss:6.7261e-01 | 0m 0.23s\n",
      "\n",
      "153.0 201.0\n",
      "VALID Epoch:  17 | Acc:   76| Loss:4.2664e-01 | 0m 0.41s\n",
      "\n",
      "##################################################\n",
      "660.0 802.0\n",
      "TRAIN Epoch:  18 | Acc:   82| Loss:7.4048e-01 | 0m 0.22s\n",
      "\n",
      "155.0 201.0\n",
      "VALID Epoch:  18 | Acc:   77| Loss:4.1748e-01 | 0m 0.41s\n",
      "\n",
      "##################################################\n",
      "673.0 802.0\n",
      "TRAIN Epoch:  19 | Acc:   83| Loss:3.9180e-01 | 0m 0.19s\n",
      "\n",
      "153.0 201.0\n",
      "VALID Epoch:  19 | Acc:   76| Loss:4.3747e-01 | 0m 0.37s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_train_epoch = 0.0\n",
    "best_val_epoch = 0.0\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    print('#'*50)\n",
    "\n",
    "    # Print learning rate for each epoch -- good for when you have a LR scheduler\n",
    "    #scheduler.step()\n",
    "    #lr = optim.param_groups[0]['lr']\n",
    "    #print('lr = %.7f' % lr)\n",
    "\n",
    "    # Save time to calculate how long it took\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Append emtpy list so that each epoch has a list of values\n",
    "    for key_ in metrics.keys():\n",
    "        metrics[key_].append([])\n",
    "\n",
    "    predictions_val = []\n",
    "    gt_val =[]\n",
    "    epoch_acc_val = []\n",
    "    epoch_acc_train = []\n",
    "\n",
    "\n",
    "    # Go through each phase TRAIN/VALID\n",
    "    #####################################################  \n",
    "    for phase in ['train','valid']:\n",
    "\n",
    "        total_correct = 0.0\n",
    "        total_size = 0.0\n",
    "\n",
    "        # Go through each data point in either train or validation\n",
    "        #####################################################  \n",
    "        for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "\n",
    "            \n",
    "            # Calculates step\n",
    "            length_dataloader = len(dataloaders[phase])\n",
    "            step = epoch * length_dataloader + i + 1\n",
    "\n",
    "            # Fetch batch of data\n",
    "            ##################################################\n",
    "            # Image data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Label data\n",
    "            lab_gt = labels.to(device)\n",
    "\n",
    "            # TRAIN\n",
    "            ##################################################\n",
    "            if phase == 'train':\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                outputs =  net(inputs).squeeze()  \n",
    "\n",
    "                current_loss =  loss(outputs,labels) \n",
    "\n",
    "                #####\n",
    "                metrics['loss_train'][-1].append(current_loss.item())\n",
    "                writer.add_scalar('loss/train', current_loss.item(), epoch*length_dataloader+i)\n",
    "\n",
    "                predicted = torch.round(torch.nn.functional.sigmoid(outputs))\n",
    "                correct_batch = (predicted == lab_gt).sum().item()\n",
    "\n",
    "                total_correct += correct_batch\n",
    "                total_size += lab_gt.shape[0]\n",
    "\n",
    "                accuracy_batch = correct_batch / lab_gt.shape[0]\n",
    "                epoch_acc_train.append(accuracy_batch*100.)\n",
    "                metrics['acc_train'][-1].append(accuracy_batch * 100.)     \n",
    "                writer.add_scalar('accuracy/train', accuracy_batch * 100., epoch*length_dataloader+i)\n",
    "\n",
    "                #####\n",
    "                current_loss.backward() \n",
    "\n",
    "                optimiser.step()\n",
    "        \n",
    "            if phase == 'valid':\n",
    "\n",
    "                net.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    outputs_val = net(inputs).squeeze()\n",
    "\n",
    "                    current_loss = loss(outputs_val, lab_gt)\n",
    "\n",
    "                    metrics['loss_valid'][-1].append(current_loss.item())\n",
    "                    writer.add_scalar('loss/val', current_loss.item(), epoch*length_dataloader+i)\n",
    "\n",
    "                    predicted_val = torch.round(torch.nn.functional.sigmoid(outputs_val))\n",
    "                    predictions_val = predictions_val + list(predicted_val.numpy())\n",
    "                    gt_val = gt_val + list(lab_gt.numpy())\n",
    "\n",
    "                    correct_batch = (predicted_val == lab_gt).sum().item()\n",
    "                    total_correct += correct_batch\n",
    "                    total_size += lab_gt.shape[0]\n",
    "\n",
    "                    accuracy_batch = correct_batch / lab_gt.shape[0]\n",
    "                    epoch_acc_val.append(accuracy_batch*100.)\n",
    "                    metrics['acc_valid'][-1].append(accuracy_batch * 100.)  \n",
    "                    writer.add_scalar('accuracy/val', accuracy_batch * 100., epoch*length_dataloader+i)   \n",
    "\n",
    "        if phase =='valid':\n",
    "            if np.mean(epoch_acc_val)>best_val_epoch:\n",
    "                best_val_epoch=np.mean(epoch_acc_val)\n",
    "            writer.add_scalar('accuracy_best_epoch/val',best_val_epoch, epoch)   \n",
    "        if phase =='train':\n",
    "            if np.mean(epoch_acc_train)>best_train_epoch:\n",
    "                best_train_epoch=np.mean(epoch_acc_train)\n",
    "            writer.add_scalar('accuracy_best_epoch/train',best_train_epoch, epoch)   \n",
    "\n",
    "\n",
    "        # PRINT STATS\n",
    "        ###################################################\n",
    "        print(total_correct, total_size)\n",
    "        accuracy = total_correct / total_size\n",
    "\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(\"%s Epoch: %3d | Acc:  %3d| Loss:%.4e | %.0fm %.2fs\" %\n",
    "                (phase.upper(), epoch, accuracy * 100,\n",
    "                current_loss.item(), time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_hparams(hparam_dict={'batch_size':batch_size}, metric_dict={'loss':np.array(metrics['loss_train'][0][0]).reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "len(metrics['loss_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}